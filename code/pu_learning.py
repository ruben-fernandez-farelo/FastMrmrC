from typing import Literal, Union
import numpy as np
from code.data_processing import get_data_features
from sklearn.metrics import pairwise_distances
from code.models import get_model, set_class_weights
from sklearn.ensemble import RandomForestClassifier

distances = None


def compute_pairwise_jaccard_measures(x):
    """
    Compute pairwise Jaccard distances between the genes in the dataset. The distances are stored in a global variable.

    Parameters
    ----------
    - x : np.ndarray
        - The fabricated ids of the genes in the dataset.

    """

    global distances

    x = x.to_numpy().astype(bool)

    distances = pairwise_distances(x, metric="jaccard")
    distances[np.diag_indices_from(distances)] = np.nan


def feature_selection_jaccard(
    x,
    y,
    n_features: int = 0,
    random_state=42,
    classifier: Union[Literal["EEC", "BRF", "CAT", "XGB"], None] = None,
):
    """
    Recompute pairwise Jaccard distances between genes using only the best predictive feature, to avoid dimensionality issues.
    The provided classifier is trained on the dataset and its n_features most important features are selected.
    The pairwise Jaccard distances are then recomputed using only these features.

    Parameters
    ----------
    - x : np.ndarray
        - The fabricated ids of the genes in the dataset.
    - y : np.ndarray
        - The target labels.
    - n_features : int
        - The number of features to select.
        - If 0, all features will be used (for compatibility purposes, i.e. no feature selection will be done).
    - random_state : int
    - classifier : str (one of "EEC", "BRF", "CAT", "XGB") | None
        - The classifier to use for feature selection (default: None)

    """

    x_feat = get_data_features(x)

    if n_features > 1:

        model = get_model(classifier, random_state=random_state)

        if classifier == "CAT":
            model = set_class_weights(model, y)
            model.fit(x_feat, y, verbose=0)
        else:
            model.fit(x_feat, y)

        feature_importances = model.feature_importances_

        idx = np.argsort(feature_importances)[::-1][:n_features]

        x_feat = x_feat.iloc[:, idx]

        compute_pairwise_jaccard_measures(x_feat)


def select_reliable_negatives(
    x: np.ndarray,
    y: np.ndarray,
    method: Literal["similarity", "threshold"],
    k: int,
    t: float,
    random_state: int,
) -> tuple:
    """
    Select reliable negative samples based on the given method.

    Parameters
    ----------

    - x (array-like): The input fabricated ids of training genes.
    - y (array-like): The target labels.

    - method (str): The method to use for selecting reliable negatives. Can be either 'similarity' or 'threshold'.
      - 'similarity': Select reliable negatives based a knn approach over precomputed pairwise jaccard distances.
      - 'threshold': Select reliable negatives based on the class probabilities generated by a random forest model.

    - k (int):
      - For 'similarity': The number of closest genes to consider.
      - For 'threshold': The number of subsets to split the unlabelled data into.


    - t (float):
      - For 'similarity': The minimum proportion of unlabelled genes among closest genes to label a gene as negative.
      - For 'threshold': The maximum probability of a gene being positive to label it as negative.

    Returns
    -------
    - x (array-like): The fabricated ids of the selected reliable negative samples.
    - y (array-like): The target labels of the selected reliable negative samples.
    """

    global distances

    k, t = int(k), float(t)

    assert 0 <= t <= 1, "t must be between 0 and 1"
    assert k > 0, "k must be greater than 0"

    p = x[y == 1]
    u = x[y == 0]


    # A)) Similarity-based approach
    # 1) For each unlabelled gene, find the k closest genes based on precomputed pairwise jaccard distances
    # 2) A gene is labelled as a reliable negative if
    #    a) The closest gene is unlabelled
    #    b) The proportion of unlabelled genes among the k closest genes is greater than or equal to t
    if method == "similarity":

        distances_subset = distances.copy()

        distances_subset[
            :,
            ~np.isin(np.arange(distances_subset.shape[1]), x),
        ] = np.nan  # This avoids leakage of info from the val/test sets (not considered in the neighbourhood search)

        topk = np.argsort(distances_subset, axis=1)[:, :k]

        topk_is_unlabelled = np.isin(topk, u)
        closest_unlabelled = topk_is_unlabelled[
            :, 0
        ]  # Condition 1: Closest gene is unlabelled
        topk_percent_unlabelled = np.mean(topk_is_unlabelled, axis=1) >= t # Condition 2: Proportion of unlabelled genes among the k closest genes is >= t


        rn = np.where((closest_unlabelled & (topk_percent_unlabelled)))[0]
        rn = np.intersect1d(rn, u) # This avoid leakage of info from the val/test sets (not considered for reliable negatives)

    # B)) Threshold-based approach
    # 1) Split the unlabelled data into k subsets
    # 2) Train a random forest model on the positive samples and the i-th subset of unlabelled samples
    # 3) Label as reliable negatives the genes in the i-th subset with a probability of being positive <= t
    # 4) Repeat for all k subsets
    # 5) Return the selected reliable negatives and the positives as the new training set
    elif method == "threshold":

        u = u[np.random.permutation(len(u))]

        u_split = np.array_split(u, k)

        rn = []

        for i in range(k):

            # Train a random forest on the positive samples and the i-th subset of unlabelled samples
            x_i = np.concatenate([p, u_split[i]])
            y_i = np.concatenate([np.ones(len(p)), np.zeros(len(u_split[i]))])

            x_i_feat = get_data_features(x_i)

            model = RandomForestClassifier(
                random_state=random_state, min_samples_leaf=5
            )
            model.fit(x_i_feat, y_i)

            u_i_feat = get_data_features(u_split[i])

            model_predictions = model.predict_proba(u_i_feat)[:, 1]

            rn.append(u_split[i][model_predictions <= t])

        rn = np.concatenate(rn)

    x = np.concatenate([p, rn])
    y = np.concatenate([np.ones(len(p)), np.zeros(len(rn))])

    idx = np.arange(len(y))
    np.random.shuffle(idx)
    x = x[idx]
    y = y[idx]

    return x, y
